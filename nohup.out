[2023-09-27 09:33:57,027] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
09/27/2023 09:34:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 4distributed training: False, 16-bits training: True
09/27/2023 09:34:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=4,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=results/pythia-160m_LAMA_TREx_prompt_tuning/P36/runs/Sep27_09-34-00_Markov,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=results/pythia-160m_LAMA_TREx_prompt_tuning/P36,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=128,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=results/pythia-160m_LAMA_TREx_prompt_tuning/P36,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=no,
save_total_limit=None,
seed=0,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Traceback (most recent call last):
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/data/cwkang/factual_knowledge_probing/src/factual_knowledge_probing/run_factual_knowledge_probing.py", line 566, in <module>
    main()
  File "/data/cwkang/factual_knowledge_probing/src/factual_knowledge_probing/run_factual_knowledge_probing.py", line 217, in main
    raw_datasets = load_dataset(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/load.py", line 2129, in load_dataset
    builder_instance = load_dataset_builder(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/load.py", line 1815, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/load.py", line 1424, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/load.py", line 958, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/data_files.py", line 674, in from_patterns
    DataFilesList.from_patterns(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/data_files.py", line 579, in from_patterns
    resolve_pattern(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/data_files.py", line 368, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/data/cwkang/factual_knowledge_probing/./data/LAMA_TREx/train_relation_wiseP36.json'
[2023-09-27 09:35:16,720] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
09/27/2023 09:35:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
09/27/2023 09:35:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=results/pythia-160m_LAMA_TREx_prompt_tuning/P36/runs/Sep27_09-35-19_Markov,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=results/pythia-160m_LAMA_TREx_prompt_tuning/P36,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=128,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=results/pythia-160m_LAMA_TREx_prompt_tuning/P36,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=no,
save_total_limit=None,
seed=0,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Traceback (most recent call last):
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/data/cwkang/factual_knowledge_probing/src/factual_knowledge_probing/run_factual_knowledge_probing.py", line 566, in <module>
    main()
  File "/data/cwkang/factual_knowledge_probing/src/factual_knowledge_probing/run_factual_knowledge_probing.py", line 217, in main
    raw_datasets = load_dataset(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/load.py", line 2129, in load_dataset
    builder_instance = load_dataset_builder(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/load.py", line 1815, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/load.py", line 1424, in dataset_module_factory
    return PackagedDatasetModuleFactory(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/load.py", line 958, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/data_files.py", line 674, in from_patterns
    DataFilesList.from_patterns(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/data_files.py", line 579, in from_patterns
    resolve_pattern(
  File "/data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/data_files.py", line 368, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/data/cwkang/factual_knowledge_probing/./data/LAMA_TREx/train_relation_wiseP36.json'
[2023-09-27 09:36:04,059] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
09/27/2023 09:36:07 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
09/27/2023 09:36:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=results/pythia-160m_LAMA_TREx_prompt_tuning/P36/runs/Sep27_09-36-07_Markov,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=results/pythia-160m_LAMA_TREx_prompt_tuning/P36,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=128,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=results/pythia-160m_LAMA_TREx_prompt_tuning/P36,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=no,
save_total_limit=None,
seed=0,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using custom data configuration default-dfad3f7efb497919
09/27/2023 09:36:08 - INFO - datasets.builder - Using custom data configuration default-dfad3f7efb497919
Loading Dataset Infos from /data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/packaged_modules/json
09/27/2023 09:36:08 - INFO - datasets.info - Loading Dataset Infos from /data2/cwkang/anaconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
09/27/2023 09:36:08 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/cwkang/.cache/huggingface/datasets/json/default-dfad3f7efb497919/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
09/27/2023 09:36:08 - INFO - datasets.info - Loading Dataset info from /home/cwkang/.cache/huggingface/datasets/json/default-dfad3f7efb497919/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
Found cached dataset json (/home/cwkang/.cache/huggingface/datasets/json/default-dfad3f7efb497919/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
09/27/2023 09:36:08 - INFO - datasets.builder - Found cached dataset json (/home/cwkang/.cache/huggingface/datasets/json/default-dfad3f7efb497919/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Loading Dataset info from /home/cwkang/.cache/huggingface/datasets/json/default-dfad3f7efb497919/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
09/27/2023 09:36:08 - INFO - datasets.info - Loading Dataset info from /home/cwkang/.cache/huggingface/datasets/json/default-dfad3f7efb497919/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
[INFO|configuration_utils.py:715] 2023-09-27 09:36:08,881 >> loading configuration file config.json from cache at /home/cwkang/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/config.json
[INFO|configuration_utils.py:775] 2023-09-27 09:36:08,883 >> Model config GPTNeoXConfig {
  "_name_or_path": "EleutherAI/pythia-160m",
  "architectures": [
    "GPTNeoXForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classifier_dropout": 0.1,
  "eos_token_id": 0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neox",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "rope_scaling": null,
  "rotary_emb_base": 10000,
  "rotary_pct": 0.25,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.33.2",
  "use_cache": true,
  "use_parallel_residual": true,
  "vocab_size": 50304
}

[INFO|tokenization_utils_base.py:1852] 2023-09-27 09:36:09,106 >> loading file vocab.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-09-27 09:36:09,106 >> loading file merges.txt from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-09-27 09:36:09,106 >> loading file tokenizer.json from cache at /home/cwkang/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-09-27 09:36:09,107 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-09-27 09:36:09,107 >> loading file special_tokens_map.json from cache at /home/cwkang/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/special_tokens_map.json
[INFO|tokenization_utils_base.py:1852] 2023-09-27 09:36:09,107 >> loading file tokenizer_config.json from cache at /home/cwkang/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/tokenizer_config.json
[INFO|modeling_utils.py:2869] 2023-09-27 09:36:09,178 >> loading weights file model.safetensors from cache at /home/cwkang/.cache/huggingface/hub/models--EleutherAI--pythia-160m/snapshots/50f5173d932e8e61f858120bcb800b97af589f46/model.safetensors
[INFO|configuration_utils.py:768] 2023-09-27 09:36:09,186 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 0,
  "transformers_version": "4.33.2"
}

[INFO|modeling_utils.py:3655] 2023-09-27 09:36:10,359 >> All model checkpoint weights were used when initializing GPTNeoXForCausalLM.

[INFO|modeling_utils.py:3663] 2023-09-27 09:36:10,360 >> All the weights of GPTNeoXForCausalLM were initialized from the model checkpoint at EleutherAI/pythia-160m.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoXForCausalLM for predictions without further training.
[INFO|modeling_utils.py:3232] 2023-09-27 09:36:10,580 >> Generation config file not found, using a generation config created from the model config.
[ERROR|tokenization_utils_base.py:1061] 2023-09-27 09:36:10,596 >> Using pad_token, but it is not set yet.
[ERROR|tokenization_utils_base.py:1061] 2023-09-27 09:36:10,596 >> Using pad_token, but it is not set yet.
[INFO|tokenization_utils_base.py:926] 2023-09-27 09:36:10,596 >> Assigning [PAD] to the pad_token key of the tokenizer
[WARNING|modeling_utils.py:1519] 2023-09-27 09:36:10,597 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50278. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trainable params: 220,160 || all params: 162,503,168 || trainable%: 0.13548043567987547
None
09/27/2023 09:36:11 - WARNING - root - Loading data...
09/27/2023 09:36:11 - WARNING - root - Formatting inputs...
09/27/2023 09:36:11 - WARNING - root - Tokenizing inputs... This may take some time...
09/27/2023 09:36:11 - WARNING - root - Loading data...
09/27/2023 09:36:11 - WARNING - root - Formatting inputs...
09/27/2023 09:36:11 - WARNING - root - Tokenizing inputs... This may take some time...
[INFO|trainer.py:1712] 2023-09-27 09:36:14,105 >> ***** Running training *****
[INFO|trainer.py:1713] 2023-09-27 09:36:14,105 >>   Num examples = 330
[INFO|trainer.py:1714] 2023-09-27 09:36:14,105 >>   Num Epochs = 3
[INFO|trainer.py:1715] 2023-09-27 09:36:14,105 >>   Instantaneous batch size per device = 128
[INFO|trainer.py:1718] 2023-09-27 09:36:14,105 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:1719] 2023-09-27 09:36:14,105 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1720] 2023-09-27 09:36:14,105 >>   Total optimization steps = 9
[INFO|trainer.py:1721] 2023-09-27 09:36:14,105 >>   Number of trainable parameters = 220,160
  0%|          | 0/9 [00:00<?, ?it/s][WARNING|modeling_gpt_neox.py:625] 2023-09-27 09:36:14,713 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
 11%|█         | 1/9 [00:01<00:08,  1.11s/it][WARNING|modeling_gpt_neox.py:625] 2023-09-27 09:36:15,223 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
 22%|██▏       | 2/9 [00:01<00:05,  1.35it/s][WARNING|modeling_gpt_neox.py:625] 2023-09-27 09:36:15,709 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
 33%|███▎      | 3/9 [00:01<00:03,  1.86it/s][WARNING|modeling_gpt_neox.py:625] 2023-09-27 09:36:16,006 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
 44%|████▍     | 4/9 [00:02<00:02,  1.93it/s][WARNING|modeling_gpt_neox.py:625] 2023-09-27 09:36:16,494 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
 56%|█████▌    | 5/9 [00:02<00:02,  1.97it/s][WARNING|modeling_gpt_neox.py:625] 2023-09-27 09:36:16,980 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
 67%|██████▋   | 6/9 [00:03<00:01,  2.31it/s][WARNING|modeling_gpt_neox.py:625] 2023-09-27 09:36:17,272 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
 78%|███████▊  | 7/9 [00:03<00:00,  2.22it/s][WARNING|modeling_gpt_neox.py:625] 2023-09-27 09:36:17,759 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
 89%|████████▉ | 8/9 [00:04<00:00,  2.16it/s][WARNING|modeling_gpt_neox.py:625] 2023-09-27 09:36:18,248 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
100%|██████████| 9/9 [00:04<00:00,  2.44it/s][INFO|trainer.py:1960] 2023-09-27 09:36:18,537 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                             100%|██████████| 9/9 [00:04<00:00,  2.44it/s]100%|██████████| 9/9 [00:04<00:00,  2.03it/s]
[INFO|trainer.py:2841] 2023-09-27 09:36:18,542 >> Saving model checkpoint to results/pythia-160m_LAMA_TREx_prompt_tuning/P36
[INFO|tokenization_utils_base.py:2235] 2023-09-27 09:36:18,546 >> tokenizer config file saved in results/pythia-160m_LAMA_TREx_prompt_tuning/P36/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-09-27 09:36:18,546 >> Special tokens file saved in results/pythia-160m_LAMA_TREx_prompt_tuning/P36/special_tokens_map.json
{'train_runtime': 4.4316, 'train_samples_per_second': 223.397, 'train_steps_per_second': 2.031, 'train_loss': 7.1924692789713545, 'epoch': 3.0}
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     7.1925
  train_runtime            = 0:00:04.43
  train_samples            =        330
  train_samples_per_second =    223.397
  train_steps_per_second   =      2.031
09/27/2023 09:36:18 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3115] 2023-09-27 09:36:18,574 >> ***** Running Prediction *****
[INFO|trainer.py:3117] 2023-09-27 09:36:18,574 >>   Num examples = 141
[INFO|trainer.py:3120] 2023-09-27 09:36:18,574 >>   Batch size = 128
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 55.60it/s]
Processing output predictions...
0it [00:00, ?it/s]5it [00:00, 48.30it/s]11it [00:00, 50.10it/s]17it [00:00, 50.00it/s]23it [00:00, 50.31it/s]29it [00:00, 47.99it/s]35it [00:00, 48.72it/s]41it [00:00, 49.45it/s]47it [00:00, 49.81it/s]53it [00:01, 50.05it/s]59it [00:01, 50.24it/s]65it [00:01, 49.62it/s]71it [00:01, 49.79it/s]76it [00:01, 49.38it/s]82it [00:01, 49.93it/s]88it [00:01, 50.25it/s]94it [00:01, 49.68it/s]99it [00:01, 49.33it/s]104it [00:02, 49.01it/s]109it [00:02, 48.26it/s]114it [00:02, 48.41it/s]119it [00:02, 48.35it/s]125it [00:02, 48.98it/s]130it [00:02, 49.00it/s]136it [00:02, 49.43it/s]141it [00:02, 49.53it/s]141it [00:02, 49.38it/s]
[INFO|modelcard.py:452] 2023-09-27 09:36:24,820 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** test metrics *****
  perplexity              =  1733.8966
  test_loss               =     7.4581
  test_runtime            = 0:00:00.22
  test_samples            =        141
  test_samples_per_second =    621.522
  test_steps_per_second   =      8.816
